\documentclass[12pt]{article}

%% FONTS
%% To get the default sans serif font in latex, uncomment following line:
 \renewcommand*\familydefault{\sfdefault}
%%
%% to get Arial font as the sans serif font, uncomment following line:
%% \renewcommand{\sfdefault}{phv} % phv is the Arial font
%%
%% to get Helvetica font as the sans serif font, uncomment following line:
% \usepackage{helvet}
\usepackage[small,bf,up]{caption}
\renewcommand{\captionfont}{\footnotesize}
\usepackage[left=1in,right=1in,top=1in,bottom=1in]{geometry}
\usepackage{graphics,epsfig,graphicx,float,subfigure,color}
\usepackage{amsmath,amssymb,amsbsy,amsfonts,amsthm}
\usepackage{url}
\usepackage{boxedminipage}
\usepackage[sf,bf,tiny]{titlesec}
 \usepackage[plainpages=false, colorlinks=true,
   citecolor=blue, filecolor=blue, linkcolor=blue,
   urlcolor=blue]{hyperref}
\usepackage{enumitem}
\usepackage{verbatim}
\usepackage{tikz,pgfplots}

\newcommand{\todo}[1]{\textcolor{red}{#1}}
% see documentation for titlesec package
% \titleformat{\section}{\large \sffamily \bfseries}
\titlelabel{\thetitle.\,\,\,}

\newcommand{\bs}{\boldsymbol}
\newcommand{\alert}[1]{\textcolor{red}{#1}}
\setlength{\emergencystretch}{20pt}

\begin{document}

\begin{center}
  \vspace*{-2cm}
{\small MATH-GA 2012.001 and CSCI-GA 2945.001, Georg Stadler \&
  Dhairya Malhotra (NYU Courant)}
\end{center}
\vspace*{.5cm}
\begin{center}
\large \textbf{%%
Spring 2019: Advanced Topics in Numerical Analysis: \\
High Performance Computing \\
Assignment 4 (due Apr.\ 15, 2019) }
\end{center}

\vspace*{0.4cm}
\begin{center}
\large \textbf{Terrence Alsup}
\end{center}

\vspace*{0.25cm}

% The machine used.
%\emph{Note: }

\vspace*{0.25cm}

% ****************************
\begin{enumerate}
% --------------------------


\item {\bf Matrix-vector operations on a GPU.} 

\par The CUDA file {\tt reduction.cu} contains a parallelized dot product and matrix-vector product.  The table below shows the memory bandwidth on different GPUs (cuda\{1-5\}.cims.nyu.edu) and a CPU.  Interestingly, the cuda3 compute server was by far the fastest and we obtained significant speed-up for the matrix-vector product.  cuda4 and cuda5 compute servers were significantly slower.  For the dot product, the problem was small enough that it was more costly to transfer memory between the device and the host than actually performing all of the floating point operations.  This is why we do not see any speed-up for this problem.

\begin{table}[H]
\centering
\begin{tabular}{| l | r | r|}
\hline
Processor & Dot Product & Matrix-Vector Product \\
\hline
CPU &  1.02 & 29.47 \\
\hline
cuda1 & 0.10 & 51.45 \\
\hline
cuda2 & 0.26 & 95.99 \\
\hline
cuda3 & 0.22 & 184.26\\
\hline
cuda4 & 0.05 & 5.08 \\
\hline
cuda5 & 0.01 & 3.38 
\\
\hline
\end{tabular}
\caption{The memory bandwidth GB/s of the dot product and matrix-vector product for the CPU and different GPUs.  Here $N = 2^{10}$.  The CPU reference values were taken when using the cuda1 compute server.}
\end{table}

  
\item {\bf 2D Jacobi method on a GPU.}

\par The CUDA file {\tt jacobi2D.cu} contains an implementation of the 2D Jacobi method to solve Poisson's equation from Homework 2.  The table below shows the timings on different GPUs as well as the CPU result from the OpenMP implementation from Homework 2.  We see significant speedup for cuda1, cuda2, and cuda3 compute servers with cuda3 again being the fastest.  Also as before, cuda4 and cuda5 are much slower and perform about the same as the CPU with OpenMP.
  
\begin{table}[H]
\centering
\begin{tabular}{| l | r |}
\hline
Processor & Wall-clock time (s) \\
\hline
CPU &  17.62  \\
\hline
cuda1 &  0.50 \\
\hline
cuda2 &  0.35 \\
\hline
cuda3 & 0.33  \\
\hline
cuda4 & 14.56 \\
\hline
cuda5 &  21.19\\
\hline
\end{tabular}
\caption{Wall-clock time in (s) to perform $10^{4}$ iterations of Jacobi's method when $N = 2^7 = 128$.  Note that for the CPU result we actually used $N=100$.  For the CPU with OpenMP implementation 4 threads were used.}
\end{table}

\item {\bf Pitch your final project.}  \\

\par  Anya and I will work on parallelizing code from an old research project that implements Kinetic Monte Carlo (KMC) in serial.  KMC is an algorithm for modelling the dynamics of a continuous time jump Markov process. The process we will consider is atom movement on the surface of a crystal.  In 2D we consider a lattice $\Omega = \{1,\ldots,L\} \times \{1,\ldots,L\}$.  At each site $\vec{\ell} \in \Omega$ on the lattice we have atoms stacked on top of each other.  The number of atoms at each site $\vec{\ell}$ will be denoted by the height of the stack $h_{\vec{\ell}}$.  The surface of the crystal is then just the atoms at the top of each stack.  KMC simulates a Markov jump process describing the evolution of the vector $(h_{\vec{\ell}}(t))_{\vec{\ell} \in \Omega}$ by randomly choosing one lattice site, at which the topmost atom will jump to the top of one of the neighboring stacks. The rates of this process are governed by the height difference between two neighboring stacks.  In the serial implementation of KMC, only one jump occurs at a time, and all other atoms, even those far away, must wait before getting a chance to move. \\

\par To parallelize KMC we will divide the lattice into sections and then independently run Markov jump processes on each section.  After every process has run for a while we will need to synchronize the sections since the jump times are random.  We will also need to handle potential conflicts on the boundaries of the sections where atoms may be jumping across.  This could potentially be handled with a layer of ghost lattice sites.  We plan to transfer the computation to a GPU for the parallelization.  \\

\par For large values of $L$, this Markov process can be seen as a discretized version of continuous dynamics (described by a PDE) governing the evolution of a smooth surface $h(x, t)$. This PDE is known and solutions to it can be computed numerically. To test our implementation, we can evolve a continuous height profile forward for a certain amount of time using the PDE, evolve a discretization of that profile for the same amount of time using parallel KMC, and compare the two resulting profiles. 




\end{enumerate}

\end{document}
